{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안녕하세요^^ \n",
    "# AIVLE 미니 프로젝트에 오신 여러분을 환영합니다.\n",
    "* 본 과정에서는 실제 사례와 데이터를 기반으로 문제를 해결하는 전체 과정을 자기 주도형 실습으로 진행해볼 예정입니다.\n",
    "* 앞선 교육과정을 정리하는 마음과 지금까지 배운 내용을 바탕으로 문제 해결을 해볼게요!\n",
    "* 미니 프로젝트를 통한 문제 해결 과정 'A에서 Z까지', 지금부터 시작합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "### reference\n",
    "> * [Google guide](https://developers.google.com/machine-learning/guides/text-classification/step-3)\n",
    "> * N-grams\n",
    ">> * [scikit-learn working with text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#)\n",
    ">> * [scikit-learn text feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    ">> * [한글 자료](https://datascienceschool.net/03%20machine%20learning/03.01.03%20Scikit-Learn%EC%9D%98%20%EB%AC%B8%EC%84%9C%20%EC%A0%84%EC%B2%98%EB%A6%AC%20%EA%B8%B0%EB%8A%A5.html)\n",
    "> * Sequence\n",
    ">> * [keras text classification](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
    ">> * [tensorflow text classification](https://www.tensorflow.org/tutorials/keras/text_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 라이브러리 설치 및 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "## import sklearn\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mecab = Mecab()\n",
    "#fm.findSystemFonts()\n",
    "plt.rcParams['font.family']= [\"NanumGothicCoding\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False\n",
    "# GPU 환경 설정하기\n",
    "# assert tf.test.is_gpu_available() == True, 'GPU 설정을 확인하세요.'\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.  데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Web발신]골든브릿지에서손실을보셨나요??http://bitly.kr/bRGtq[FW]</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Web발신][대//박]가(.원)입코(WN)드MEP.com</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Web발신](광고)이정미님아직도주식하시나요http://pf.kakao.com/_u...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Web발신]송승용님사람이모이는곳에는이유가있습니다하루~정보공개is.gd/JsJP</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label\n",
       "0        [Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA  spam\n",
       "1   [Web발신]골든브릿지에서손실을보셨나요??http://bitly.kr/bRGtq[FW]  spam\n",
       "2                   [Web발신][대//박]가(.원)입코(WN)드MEP.com  spam\n",
       "3  [Web발신](광고)이정미님아직도주식하시나요http://pf.kakao.com/_u...  spam\n",
       "4       [Web발신]송승용님사람이모이는곳에는이유가있습니다하루~정보공개is.gd/JsJP  spam"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터를 가져옵니다.\n",
    "train_filename = '/aihub/workspace/spam.csv'\n",
    "data = pd.read_csv(train_filename)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1-1. processing label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 데이터를 수치형으로 변환합니다.\n",
    "data['label'] = data['label'].map({'spam' : 1, \"ham\" : 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 20089 entries, 0 to 20099\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    20089 non-null  object\n",
      " 1   label   20089 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 470.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(['text'], keep='first', inplace=False, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18386 entries, 0 to 20099\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    18386 non-null  object\n",
      " 1   label   18386 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 430.9+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Train Validation(Test) Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train validation set으로 분리합니다.\n",
    "x_train, x_val, y_train, y_val = train_test_split(data['text'], data['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14708,), (3678,), (14708,), (3678,))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vectorize texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. N-grams Vectorize [참고](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_x_train = count_vect.fit_transform(x_train)\n",
    "cv_x_val = count_vect.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14708, 29499), (3678, 29499))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_x_train.shape, cv_x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4966)\t1\n",
      "  (0, 9499)\t1\n",
      "  (0, 25913)\t1\n",
      "  (0, 2696)\t1\n",
      "  (0, 4249)\t1\n",
      "  (1, 4966)\t1\n",
      "  (1, 23362)\t1\n",
      "  (1, 1771)\t1\n",
      "  (1, 3997)\t1\n",
      "  (1, 2080)\t1\n",
      "  (1, 675)\t1\n",
      "  (1, 49)\t1\n",
      "  (2, 4966)\t1\n",
      "  (2, 3127)\t1\n",
      "  (2, 6992)\t1\n",
      "  (2, 5336)\t1\n",
      "  (2, 14829)\t1\n",
      "  (2, 18527)\t1\n",
      "  (2, 20972)\t1\n",
      "  (2, 20971)\t1\n",
      "  (2, 8723)\t1\n",
      "  (3, 2696)\t1\n",
      "  (3, 11704)\t1\n",
      "  (3, 1662)\t1\n",
      "  (3, 408)\t1\n",
      "  :\t:\n",
      "  (14704, 28838)\t1\n",
      "  (14704, 6027)\t1\n",
      "  (14704, 7191)\t1\n",
      "  (14704, 10001)\t1\n",
      "  (14705, 3127)\t1\n",
      "  (14705, 6992)\t1\n",
      "  (14705, 17639)\t1\n",
      "  (14705, 6595)\t1\n",
      "  (14705, 15777)\t1\n",
      "  (14705, 2915)\t1\n",
      "  (14705, 15019)\t1\n",
      "  (14705, 7606)\t1\n",
      "  (14705, 23391)\t1\n",
      "  (14706, 4966)\t1\n",
      "  (14706, 1769)\t1\n",
      "  (14706, 3665)\t1\n",
      "  (14706, 4663)\t1\n",
      "  (14706, 25333)\t1\n",
      "  (14706, 1123)\t1\n",
      "  (14706, 20867)\t1\n",
      "  (14707, 6992)\t1\n",
      "  (14707, 3039)\t1\n",
      "  (14707, 14233)\t1\n",
      "  (14707, 9282)\t1\n",
      "  (14707, 3723)\t1\n"
     ]
    }
   ],
   "source": [
    "print(cv_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# Vectorization parameters\n",
    "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Whether text should be split into word or character n-grams.\n",
    "# One of 'word', 'char'.\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded.\n",
    "MIN_DOCUMENT_FREQUENCY = 2\n",
    "\n",
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors.\n",
    "\n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        train_labels: np.ndarray, training labels.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "            'ngram_range': NGRAM_RANGE,  # Use 1-grams + 2-grams.\n",
    "            'dtype': 'int32',\n",
    "            'strip_accents': 'unicode',\n",
    "            'decode_error': 'replace',\n",
    "            'analyzer': TOKEN_MODE,  # Split text into word tokens.\n",
    "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:1804: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "ngram_x_train, ngram_x_val = ngram_vectorize(x_train, y_train, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14708, 10410)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. Sequence Vectorize [참고](https://developers.google.com/machine-learning/guides/text-classification/step-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.preprocessing import text\n",
    "\n",
    "# Vectorization parameters\n",
    "# Limit on the number of features. We use the top 20K features.\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences. Sequences longer than this\n",
    "# will be truncated.\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "\n",
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors.\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length.\n",
    "\n",
    "    # Arguments\n",
    "        train_texts: list, training text strings.\n",
    "        val_texts: list, validation text strings.\n",
    "\n",
    "    # Returns\n",
    "        x_train, x_val, word_index: vectorized training and validation\n",
    "            texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated\n",
    "    # at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_x_train, sequence_x_val, word_index = sequence_vectorize(x_train, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    1, 4863,  103,    3,\n",
       "          39]], dtype=int32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_x_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidVector = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_x_train = TfidVector.fit_transform(x_train)\n",
    "sequence_x_val = TfidVector.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4249)\t0.4178355550025851\n",
      "  (0, 2696)\t0.20983259916199754\n",
      "  (0, 25913)\t0.4869915088263504\n",
      "  (0, 9499)\t0.7299139404397907\n",
      "  (0, 4966)\t0.10699831394603691\n",
      "  (1, 49)\t0.42900020895547913\n",
      "  (1, 675)\t0.23828952315581392\n",
      "  (1, 2080)\t0.2663334367902144\n",
      "  (1, 3997)\t0.28063667543569043\n",
      "  (1, 1771)\t0.28698786998499026\n",
      "  (1, 23362)\t0.7183561099392967\n",
      "  (1, 4966)\t0.10530404794026392\n",
      "  (2, 8723)\t0.4233138463869956\n",
      "  (2, 20971)\t0.40598194474203847\n",
      "  (2, 20972)\t0.40598194474203847\n",
      "  (2, 18527)\t0.40598194474203847\n",
      "  (2, 14829)\t0.36405568326170157\n",
      "  (2, 5336)\t0.40598194474203847\n",
      "  (2, 6992)\t0.08268742157221205\n",
      "  (2, 3127)\t0.13526128984503297\n",
      "  (2, 4966)\t0.06205370979232093\n",
      "  (3, 408)\t0.5023297780243415\n",
      "  (3, 1662)\t0.698012919606757\n",
      "  (3, 11704)\t0.465474229394362\n",
      "  (3, 2696)\t0.20922834402105148\n",
      "  :\t:\n",
      "  (14704, 28673)\t0.4401511164176098\n",
      "  (14704, 6992)\t0.0859763063016612\n",
      "  (14704, 3127)\t0.140641295439577\n",
      "  (14704, 4966)\t0.06452189049817834\n",
      "  (14705, 23391)\t0.42393099603396645\n",
      "  (14705, 7606)\t0.42393099603396645\n",
      "  (14705, 15019)\t0.42393099603396645\n",
      "  (14705, 2915)\t0.42393099603396645\n",
      "  (14705, 15777)\t0.305241884722058\n",
      "  (14705, 6595)\t0.3349141625500351\n",
      "  (14705, 17639)\t0.22490837237484304\n",
      "  (14705, 6992)\t0.08280797164036544\n",
      "  (14705, 3127)\t0.13545848740421787\n",
      "  (14706, 20867)\t0.6363760671728721\n",
      "  (14706, 1123)\t0.3924760385528276\n",
      "  (14706, 25333)\t0.3652283971990292\n",
      "  (14706, 4663)\t0.3493945837156877\n",
      "  (14706, 3665)\t0.3486601700210253\n",
      "  (14706, 1769)\t0.2350604287580292\n",
      "  (14706, 4966)\t0.09328656770424289\n",
      "  (14707, 3723)\t0.49763222472047003\n",
      "  (14707, 9282)\t0.49763222472047003\n",
      "  (14707, 14233)\t0.49763222472047003\n",
      "  (14707, 3039)\t0.49763222472047003\n",
      "  (14707, 6992)\t0.0972042986653492\n"
     ]
    }
   ],
   "source": [
    "print(sequence_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_x_train.toarray()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-1. Save N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14708, 29499)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-grams 방식으로 vectorize한 데이터의 shape을 확인해봅니다.\n",
    "# 모델 학습시에 활용 가능하도록 전처리 데이터를 저장해보도록 하겠습니다.\n",
    "x_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2. Save sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train seq shape: (15066, 23)\n",
      "X_val seq shape: (5023, 23)\n"
     ]
    }
   ],
   "source": [
    "# Sequence 형태로 vectorize한 데이터의 shape을 확인해봅니다.\n",
    "# 모델 학습시에 활용 가능하도록 전처리 데이터를 저장해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14708, 23)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle#### 4-3. Save label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label 데이터의 shape을 확인하고 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save_data.pickle', 'wb') as f:\n",
    "    pickle.dump([data['text'], data['label']], f)\n",
    "    pickle.dump([x_train, x_val], f)\n",
    "    pickle.dump([y_train, y_val], f)\n",
    "    pickle.dump([cv_x_train, cv_x_val], f)\n",
    "    pickle.dump([ngram_x_train, ngram_x_val], f)\n",
    "    pickle.dump([sequence_x_train, sequence_x_val, word_index], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save_data.pickle', 'rb') as f:\n",
    "    test_y_train, test_y_val = pickle.load(f)\n",
    "    a = pickle.load(f)\n",
    "    b = pickle.load(f)\n",
    "    c = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14708,), (3678,))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_train.shape, test_y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18386,), (18386,))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'], data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}