{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 안녕하세요^^ \n",
    "# AIVLE 미니 프로젝트에 오신 여러분을 환영합니다.\n",
    "* 본 과정에서는 실제 사례와 데이터를 기반으로 문제를 해결하는 전체 과정을 자기 주도형 실습으로 진행해볼 예정입니다.\n",
    "* 앞선 교육과정을 정리하는 마음과 지금까지 배운 내용을 바탕으로 문제 해결을 해볼게요!\n",
    "* 미니 프로젝트를 통한 문제 해결 과정 'A에서 Z까지', 지금부터 시작합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Text Classification\n",
    "> * [Google Tutorial](https://developers.google.com/machine-learning/guides/text-classification)\n",
    "> * [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/keras/text_classification)\n",
    "> * [Keras-tutorial](https://keras.io/examples/nlp/text_classification_from_scratch/)\n",
    "> * [BERT-tutorial](https://www.tensorflow.org/text/guide/bert_preprocessing_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (0.11.2)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (4.2.0)\n",
      "Requirement already satisfied: wordcloud in /usr/local/lib/python3.6/dist-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.9.0)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from seaborn) (1.5.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.6/dist-packages (from seaborn) (3.3.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (6.2.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud) (8.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 필요 라이브러리부터 설치할께요.\n",
    "!pip install konlpy pandas seaborn gensim wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "## import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.metrics import accuracy_score,f1_score,confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mecab = Mecab()\n",
    "#fm.findSystemFonts()\n",
    "plt.rcParams['font.family']= [\"NanumGothicCoding\"]\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False\n",
    "# GPU 환경 설정하기\n",
    "# assert tf.test.is_gpu_available() == True, 'GPU 설정을 확인하세요.'\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.config.list_logical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. data preprocessing에서 저장한 결과를 불러오고 shape을 확인합니다.\n",
    "with open('save_data.pickle', 'rb') as f:\n",
    "    train_data, train_label = pickle.load(f)\n",
    "    x_train, x_val = pickle.load(f)\n",
    "    y_train, y_val = pickle.load(f)\n",
    "    cv_x_train, cv_x_val = pickle.load(f)\n",
    "    ngram_x_train, ngram_x_val = pickle.load(f)\n",
    "    sequence_x_train, sequence_x_val, word_index = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14708,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_x_train = tf_idf.fit_transform(ngram_x_train)\n",
    "tf_idf_x_val = tf_idf.transform(ngram_x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14708, 10410)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 4096)              42643456  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2048)              4196352   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 75,189,249\n",
      "Trainable params: 75,174,913\n",
      "Non-trainable params: 14,336\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "#############\n",
    "# Your Code #\n",
    "#############\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add( keras.layers.Input( shape=(tf_idf_x_train.shape[1], )))\n",
    "\n",
    "model.add( keras.layers.Dense(4096, activation='swish'))\n",
    "model.add( keras.layers.Dense(4096, activation='swish'))\n",
    "\n",
    "model.add( keras.layers.BatchNormalization())\n",
    "model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add( keras.layers.Dense(2048, activation='swish'))\n",
    "model.add( keras.layers.Dense(2048, activation='swish'))\n",
    "\n",
    "model.add( keras.layers.BatchNormalization())\n",
    "model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add( keras.layers.Dense(1024, activation='swish'))\n",
    "model.add( keras.layers.Dense(1024, activation='swish'))\n",
    "\n",
    "model.add( keras.layers.BatchNormalization())\n",
    "model.add( keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add( keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', # 관측 대상 / 아직 지정 x\n",
    "                   min_delta=0,        # 최소한 나빠지지 않으면 괜찮아\n",
    "                   patience=5,         # 성능 개선되지 않는걸 얼마나 참을래? 5번\n",
    "                   verbose=1,\n",
    "                   restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 10s 25ms/step - loss: 0.7036 - accuracy: 0.7093 - val_loss: 0.4437 - val_accuracy: 0.7420\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 9s 24ms/step - loss: 0.2829 - accuracy: 0.8728 - val_loss: 0.3061 - val_accuracy: 0.8651\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 9s 24ms/step - loss: 0.2269 - accuracy: 0.9014 - val_loss: 0.3623 - val_accuracy: 0.8705\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 9s 24ms/step - loss: 0.2141 - accuracy: 0.8969 - val_loss: 0.3564 - val_accuracy: 0.8729\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 10s 28ms/step - loss: 0.1988 - accuracy: 0.9041 - val_loss: 0.3192 - val_accuracy: 0.8620\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 9s 24ms/step - loss: 0.2068 - accuracy: 0.8995 - val_loss: 0.3409 - val_accuracy: 0.8725\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 9s 24ms/step - loss: 0.2013 - accuracy: 0.9029 - val_loss: 0.3108 - val_accuracy: 0.8664\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9cddb92160>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_idf_x_train.toarray(),\n",
    "          y_train,\n",
    "          validation_split=0.2,\n",
    "          callbacks=[es],       # Early Stopping 적용\n",
    "          verbose=1, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLP_pred = model.predict(tf_idf_x_val.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.81      0.82      1435\n",
      "         1.0       0.88      0.89      0.89      2243\n",
      "\n",
      "    accuracy                           0.86      3678\n",
      "   macro avg       0.85      0.85      0.85      3678\n",
      "weighted avg       0.86      0.86      0.86      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(np.round(NLP_pred, 0), y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(tf_idf_x_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.predict(tf_idf_x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(NB_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(NB_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(NB_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"learning_rate\" : [0.01,0.1,0.2,0.3,0.4,0.5],\n",
    "          \"max_depth\" : [25, 50, 75],\n",
    "          \"num_leaves\" : [100,300,500,900,1200],\n",
    "          \"n_estimators\" : [100, 200, 300,500,800,1000],\n",
    "          \"learning_rate\" : [0.01,0.1,0.2,0.3,0.4,0.5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_lgbm = GridSearchCV(lgbm, params, scoring ='accuracy', cv=3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 540 candidates, totalling 1620 fits\n",
      "[CV] END learning_rate=0.01, max_depth=25, n_estimators=100, num_leaves=100; total time= 2.3min\n",
      "[CV] END learning_rate=0.01, max_depth=25, n_estimators=100, num_leaves=100; total time= 1.6min\n",
      "[CV] END learning_rate=0.01, max_depth=25, n_estimators=100, num_leaves=100; total time= 3.3min\n",
      "[CV] END learning_rate=0.01, max_depth=25, n_estimators=100, num_leaves=300; total time= 3.3min\n",
      "[CV] END learning_rate=0.01, max_depth=25, n_estimators=100, num_leaves=300; total time= 2.9min\n",
      "[CV] END learning_rate=0.01, max_depth=25, n_estimators=100, num_leaves=300; total time= 4.0min\n"
     ]
    }
   ],
   "source": [
    "grid_lgbm.fit(tf_idf_x_train.toarray(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_pred = lgbm.predict(tf_idf_x_val.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.75      0.79      1561\n",
      "           1       0.83      0.89      0.86      2117\n",
      "\n",
      "    accuracy                           0.83      3678\n",
      "   macro avg       0.83      0.82      0.83      3678\n",
      "weighted avg       0.83      0.83      0.83      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(lgbm_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Machine Learning(N-grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1. Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams으로 vectorize한 데이터를 Naive Bayes 모델로 학습합니다.\n",
    "NB = MultinomialNB()\n",
    "\n",
    "NB.fit(tf_idf_x_train, y_train)\n",
    "\n",
    "NB_pred = NB.predict(tf_idf_x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NB_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8045133224578576"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(NB_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.851169530117988"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(NB_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 903  214]\n",
      " [ 505 2056]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(NB_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.72      1117\n",
      "           1       0.91      0.80      0.85      2561\n",
      "\n",
      "    accuracy                           0.80      3678\n",
      "   macro avg       0.77      0.81      0.78      3678\n",
      "weighted avg       0.83      0.80      0.81      3678\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(NB_pred, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2. 다른 모델들을 활용하여 학습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Learning(Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,  103,    3,   39],\n",
       "       [   0,    0,    0, ...,    6,    5,   49],\n",
       "       [   0,    0,    0, ..., 2191, 2192, 4865],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  199,  361,   29],\n",
       "       [   0,    0,    0, ...,   33,   31,   60],\n",
       "       [   0,    0,    0, ...,    0,    0,    2]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sequence 형태로 vectorize한 데이터를 딥러닝으로 학습합니다.\n",
    "# Tip : 처음에는 embedding이 필요합니다\n",
    "sequence_x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1. Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv1D로 학습하고 모델 성능을 검증합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM으로 학습하고 모델 성능을 검증합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pc\n",
    "import os\n",
    "import numpy as np\n",
    "# csv 모듈은 CSV 형식(쉼표로 구분된 표 형식) 데이터를 읽고 쓰는 클래스를 제공\n",
    "import csv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Version:  1.10.2+cu102\n",
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pc\n",
    "import os\n",
    "import numpy as np\n",
    "# csv 모듈은 CSV 형식(쉼표로 구분된 표 형식) 데이터를 읽고 쓰는 클래스를 제공\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "# torch 버전 확인\n",
    "print(\"Pytorch Version: \", torch.__version__)\n",
    "\n",
    "# GPU 사용 가능한지 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    # PyTorch 에게 GPU 사용할거라고 알려주기\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    print(\"There are %d GPU(s) available.\" % torch.cuda.device_count())\n",
    "    print(\"We will use the GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.25.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.62.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2021.8.28)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.53)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.6/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (8.0.4)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# transformers 팩키지 설치\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# BERT tokenizer 불러오기\n",
    "# do_lower_case : True이면 모두 소문자로 변환, False이면 대소문자 구분\n",
    "print(\"Loading BERT tokenizer...\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA\n",
      "Original:  [Web발신]골든브릿지에서손실을보셨나요??http://bitly.kr/bRGtq[FW]\n",
      "\n",
      "Tokenized:  ['[', 'web', '##ᄇ', '##ᅡ', '##ᆯ', '##ᄉ', '##ᅵ', '##ᆫ', ']', '[UNK]', ':', '/', '/', 'lc', '##o', '.', 'jp', '/', 'ea']\n",
      "Tokenized:  ['[', 'web', '##ᄇ', '##ᅡ', '##ᆯ', '##ᄉ', '##ᅵ', '##ᆫ', ']', '[UNK]', '?', '?', 'http', ':', '/', '/', 'bit', '##ly', '.', 'k', '##r', '/', 'br', '##gt', '##q', '[', 'f', '##w', ']']\n",
      "\n",
      "Token IDs:  [1031, 4773, 29996, 30006, 30022, 29997, 30019, 30021, 1033, 100, 1024, 1013, 1013, 29215, 2080, 1012, 16545, 1013, 19413]\n",
      "Token IDs:  [1031, 4773, 29996, 30006, 30022, 29997, 30019, 30021, 1033, 100, 1029, 1029, 8299, 1024, 1013, 1013, 2978, 2135, 1012, 1047, 2099, 1013, 7987, 13512, 4160, 1031, 1042, 2860, 1033]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 하나의 sentence에 대해 BertTokenizer 적용\n",
    "\n",
    "# Print the original sentence.\n",
    "print(\"Original: \", train_data[0])\n",
    "print(\"Original: \", train_data[1])\n",
    "print()\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print(\"Tokenized: \", tokenizer.tokenize(train_data[0]))\n",
    "print(\"Tokenized: \", tokenizer.tokenize(train_data[1]))\n",
    "print()\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[0])))\n",
    "print(\"Token IDs: \", tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_data[1])))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  [Web발신]박춘규회원님손절주식은그만월급배만드는법http://lco.jp/eA\n",
      "\n",
      "Token IDs:  [101, 1031, 4773, 29996, 30006, 30022, 29997, 30019, 30021, 1033, 100, 1024, 1013, 1013, 29215, 2080, 1012, 16545, 1013, 19413, 102]\n",
      "\n",
      "[CLS] token: [CLS], ID: 101\n",
      "\n",
      "[PAD] token: [PAD], ID: 0\n",
      "\n",
      "[SEP] token: [SEP], ID: 102\n",
      "\n",
      "Tokenized:  ['[CLS]', '[', 'web', '##ᄇ', '##ᅡ', '##ᆯ', '##ᄉ', '##ᅵ', '##ᆫ', ']', '[UNK]', ':', '/', '/', 'lc', '##o', '.', 'jp', '/', 'ea', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "input_ids = []\n",
    "\n",
    "# For every sentence\n",
    "for sent in train_data:\n",
    "    # 'encode' will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the '[CLS]' token to the start.\n",
    "    #   (3) Append the '[SEP]' token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   max_length : 문장의 최대길이\n",
    "    #   encoded_sent : token IDs\n",
    "    #---------------------------------------------------------------\n",
    "    #      연습 (1)     tokenizer의 encode 함수를 호출해 주세요.  \n",
    "    #---------------------------------------------------------------                \n",
    "  \n",
    "    encoded_sent = tokenizer.encode(sent, \n",
    "                                    add_special_tokens=True, # [CLS], [PAD], [SEP] 사용여부\n",
    "                                    max_length = 64)\n",
    "    \n",
    "    # Add the encoded sentence to the list\n",
    "    input_ids.append(encoded_sent)\n",
    "\n",
    "# Print train data[0]\n",
    "print(\"Original: \", train_data[0])\n",
    "print()\n",
    "print(\"Token IDs: \", input_ids[0])\n",
    "\n",
    "# Print special tokens and tokenized sentence\n",
    "print(\"\\n[CLS] token: {:}, ID: {:}\".format(tokenizer.cls_token, tokenizer.cls_token_id))\n",
    "print(\"\\n[PAD] token: {:}, ID: {:}\".format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "print(\"\\n[SEP] token: {:}, ID: {:}\".format(tokenizer.sep_token, tokenizer.sep_token_id))\n",
    "print(\"\\nTokenized: \", tokenizer.convert_ids_to_tokens(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  64\n"
     ]
    }
   ],
   "source": [
    "print(\"Max length: \", max([len(each) for each in input_ids]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.4.2\n",
      "\n",
      "Padding is done.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "\n",
    "MAXLEN = 64\n",
    "# post-sequence truncation, post-sequence padding\n",
    "# padding value 0, type of output sequences long\n",
    "input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                          maxlen=MAXLEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "\n",
    "print(\"\\nPadding is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attention masking is done.\n"
     ]
    }
   ],
   "source": [
    "# Create attention masks\n",
    "attention_masks = [] # 각 문장에 대한 attention mask 리스트를 저장\n",
    "\n",
    "# 토큰 시퀀스에서 패딩에 해당하는 부분은 0, 패딩이 아닌 부분은 1을 넣은 mask를 생성\n",
    "# 패딩 부분은 모델 내에서 Attention을 수행하지 않아 학습속도를 향상\n",
    "# For every sentence\n",
    "for sent in input_ids:\n",
    "    # Create the attention mask.\n",
    "    #  - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "    #  - If a token ID is not 0 ( > 0), then it's a real token, set the mask to 1.\n",
    "    att_mask = [int(token_id > 0) for token_id in sent]\n",
    "    \n",
    "    # Store the attention mask for this sentence.\n",
    "    attention_masks.append(att_mask)\n",
    "print(\"\\nAttention masking is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  1006  1455 30012 30025 29991 30011  1007   100   999   100  1012\n",
      "   1047  2099  1013  1057 18153  2102  2094 29995 30014 29994 30013 29991\n",
      "  30008 29996 30014   102     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0]]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_test_split(arrays, test_size, train_size, random_state, shuffle, stratify)\n",
    "#     arrays : 분할시킬 데이터\n",
    "#     test_size : 테스트 데이터셋의 비율 (default = 0.25)\n",
    "#     random_state : 데이터 셔플 시 seed value. 호출할 때마다 동일한 학습 데이터, 테스트 데이터 셋을 생성하기 위해 설정\n",
    "#     shuffle : 셔플 여부 (default = True)\n",
    "#     stratify : 지정한 Data의 비율을 유지한다. 예를 들어, Label Set인 Y가 25%의 0과 75%의 1로 이루어진 Binary Set일 때\n",
    "#     stratify=Y로 설정하면 나누어진 데이터셋들도 0과 1을 각각 25%, 75%로 유지\n",
    "# 반환값 : (학습 데이터, 테스트 데이터, 학습데이터 label, 테스트데이터 label)\n",
    "# test_size=0.1로 설정, Use 90% for training and 10% for validation\n",
    "train_inputs, valid_inputs, train_labels, valid_labels = train_test_split(input_ids, train_label, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Do the same for the masks.\n",
    "train_masks, valid_masks, _, _ = train_test_split(attention_masks, train_label, random_state=2018, test_size=0.1)\n",
    "\n",
    "# print train_inputs, valid_inputs\n",
    "# 첫번째 학습데이터, 첫번째 attention mask를 출력해서 확인\n",
    "print(train_inputs[:1])\n",
    "print(train_masks[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required data type for our model.\n",
    "# torch.tensor(data) -> tensor를 생성\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "valid_inputs = torch.tensor(valid_inputs)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "valid_masks = torch.tensor(valid_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17592    0\n",
       "11950    1\n",
       "10192    1\n",
       "5310     0\n",
       "1550     1\n",
       "        ..\n",
       "11228    1\n",
       "7845     1\n",
       "18890    1\n",
       "15855    0\n",
       "10200    0\n",
       "Name: label, Length: 16547, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2135     1\n",
       "15787    1\n",
       "9577     1\n",
       "8189     0\n",
       "436      0\n",
       "        ..\n",
       "2609     0\n",
       "13387    1\n",
       "7351     1\n",
       "11223    0\n",
       "4757     1\n",
       "Name: label, Length: 1839, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(list(train_labels))\n",
    "valid_labels = torch.tensor(list(valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.utils.data : 파이토치의 데이터 로딩 유틸리티. 데이터셋, 데이터로더, 샘플러 등을 제공함\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "#      연습 (2) train_data, valid_data의 데이터 로더를 생성해 주세요.\n",
    "#---------------------------------------------------------------   \n",
    "# Create the DataLoader for our training set.\n",
    "# torch.utils.data.TensorDataset(*tensors)\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "valid_data = TensorDataset(valid_inputs, valid_masks, valid_labels)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}